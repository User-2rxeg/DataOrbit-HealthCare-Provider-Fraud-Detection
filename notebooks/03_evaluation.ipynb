{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import libraries for visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, precision_recall_curve,\n",
    "    roc_curve, auc, precision_score, recall_score, f1_score,\n",
    "    accuracy_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load results_df from previous notebook or a saved file\n",
    "results_df = pd.read_csv('../data/model_results.csv')  # or pickle if you saved it\n"
   ],
   "id": "99ce4134b83285db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model comparison visualization\n",
    "if len(results_df) > 0:\n",
    "    # Performance metrics heatmap\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # 1. Performance heatmap\n",
    "    metrics_cols = ['Precision', 'Recall', 'F1', 'ROC_AUC', 'PR_AUC']\n",
    "    heatmap_data = results_df.set_index('Model')[metrics_cols]\n",
    "\n",
    "    sns.heatmap(heatmap_data.T, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                ax=axes[0,0], cbar_kws={'label': 'Score'})\n",
    "    axes[0,0].set_title('Model Performance Heatmap', fontweight='bold')\n",
    "\n",
    "    # 2. F1 Score comparison\n",
    "    f1_scores = results_df.sort_values('F1', ascending=True)\n",
    "    axes[0,1].barh(f1_scores['Model'], f1_scores['F1'], color='skyblue', alpha=0.7)\n",
    "    axes[0,1].set_xlabel('F1 Score')\n",
    "    axes[0,1].set_title('F1 Score Comparison', fontweight='bold')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Precision vs Recall\n",
    "    axes[1,0].scatter(results_df['Precision'], results_df['Recall'],\n",
    "                     c=results_df['F1'], cmap='viridis', s=100, alpha=0.7)\n",
    "    for i, model in enumerate(results_df['Model']):\n",
    "        axes[1,0].annotate(model,\n",
    "                          (results_df.iloc[i]['Precision'], results_df.iloc[i]['Recall']),\n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    axes[1,0].set_xlabel('Precision')\n",
    "    axes[1,0].set_ylabel('Recall')\n",
    "    axes[1,0].set_title('Precision vs Recall Trade-off', fontweight='bold')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. ROC vs PR AUC\n",
    "    axes[1,1].scatter(results_df['ROC_AUC'], results_df['PR_AUC'],\n",
    "                     c=results_df['F1'], cmap='plasma', s=100, alpha=0.7)\n",
    "    for i, model in enumerate(results_df['Model']):\n",
    "        axes[1,1].annotate(model,\n",
    "                          (results_df.iloc[i]['ROC_AUC'], results_df.iloc[i]['PR_AUC']),\n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    axes[1,1].set_xlabel('ROC AUC')\n",
    "    axes[1,1].set_ylabel('PR AUC')\n",
    "    axes[1,1].set_title('ROC AUC vs PR AUC', fontweight='bold')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Best model summary\n",
    "    best_f1_model = results_df.loc[results_df['F1'].idxmax()]\n",
    "    print(\"\\n=== BEST MODEL BY F1-SCORE ===\")\n",
    "    print(f\"Model: {best_f1_model['Model']}\")\n",
    "    print(f\"F1-Score: {best_f1_model['F1']:.4f}\")\n",
    "    print(f\"Precision: {best_f1_model['Precision']:.4f}\")\n",
    "    print(f\"Recall: {best_f1_model['Recall']:.4f}\")\n",
    "    print(f\"PR-AUC: {best_f1_model['PR_AUC']:.4f}\")\n"
   ],
   "id": "e653cf5b6ddaf516"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "y_train = pd.read_csv('../data/y_train.csv').squeeze()\n",
    "y_test = pd.read_csv('../data/y_test.csv').squeeze()\n",
    "# Load results dataframe\n",
    "results_df = pd.read_csv('../data/model_results.csv')\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "y_test = pd.read_csv('../data/y_test.csv').squeeze()  # make it Series\n",
    "\n",
    "# Load trained models\n",
    "model_names = ['Logistic_Regression', 'Random_Forest', 'Decision_Tree', 'SVM','XGBoost']\n",
    "trained_models = {name: joblib.load(f'../models/{name}.pkl') for name in model_names}"
   ],
   "id": "e2c110feb9b789df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # ROC Curves\n",
    "    for name, pipeline in trained_models.items():\n",
    "        try:\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            axes[0].plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not plot ROC for {name}: {e}\")\n",
    "\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curves Comparison', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Precision-Recall Curves\n",
    "    for name, pipeline in trained_models.items():\n",
    "        try:\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            axes[1].plot(recall, precision, label=f'{name} (AUC = {pr_auc:.3f})')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not plot PR for {name}: {e}\")\n",
    "\n",
    "    fraud_rate = y_test.mean()\n",
    "    axes[1].axhline(y=fraud_rate, color='k', linestyle='--', label=f'Random (={fraud_rate:.3f})')\n",
    "\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].set_title('Precision-Recall Curves Comparison', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"üìä Curve Analysis:\")\n",
    "    print(\"‚Ä¢ ROC curves show model discrimination ability\")\n",
    "    print(\"‚Ä¢ PR curves more informative for imbalanced data\")\n",
    "    print(\"‚Ä¢ Higher AUC indicates better performance\")\n"
   ],
   "id": "460a3270c1afe00f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Confusion Matrices for all models\n",
    "if len(results_df) > 0:\n",
    "    n_models = len(trained_models)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(4*n_models, 4))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (name, pipeline) in enumerate(trained_models.items()):\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Create confusion matrix heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{name}\\nConfusion Matrix', fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "        axes[idx].set_xticklabels(['Non-Fraud', 'Fraud'])\n",
    "        axes[idx].set_yticklabels(['Non-Fraud', 'Fraud'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed confusion matrix analysis\n",
    "    print(\"\\nüìä Confusion Matrix Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    for name, pipeline in trained_models.items():\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  True Negatives:  {tn:4d} (Correct non-fraud)\")\n",
    "        print(f\"  False Positives: {fp:4d} (Incorrect fraud alerts)\")\n",
    "        print(f\"  False Negatives: {fn:4d} (Missed fraud cases)\")\n",
    "        print(f\"  True Positives:  {tp:4d} (Correct fraud detection)\")\n",
    "        print(f\"  ‚Üí Fraud Detection Rate: {tp/(tp+fn):.1%}\")\n",
    "        print(f\"  ‚Üí False Alert Rate: {fp/(fp+tn):.1%}\")"
   ],
   "id": "ad42ae678fbd96f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature Importance Analysis\n",
    "if len(results_df) > 0:\n",
    "    print(\"=== Feature Importance Analysis ===\")\n",
    "\n",
    "    # Get the best performing model\n",
    "    best_model_name = results_df.loc[results_df['F1'].idxmax()]['Model']\n",
    "    best_pipeline = trained_models[best_model_name]\n",
    "\n",
    "    X = pd.read_csv('../data/X.csv')\n",
    "    y = pd.read_csv('../data/y.csv')\n",
    "\n",
    "    # Extract feature importance based on model type\n",
    "    feature_names = X.columns\n",
    "\n",
    "    try:\n",
    "        if hasattr(best_pipeline.named_steps['classifier'], 'feature_importances_'):\n",
    "            # Tree-based models (Random Forest, Decision Tree, XGBoost)\n",
    "            importances = best_pipeline.named_steps['classifier'].feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        elif hasattr(best_pipeline.named_steps['classifier'], 'coef_'):\n",
    "            # Linear models (Logistic Regression, SVM)\n",
    "            coefficients = best_pipeline.named_steps['classifier'].coef_[0]\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': np.abs(coefficients)  # Use absolute values for importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Feature importance not available for {best_model_name}\")\n",
    "            feature_importance = None\n",
    "\n",
    "        if feature_importance is not None:\n",
    "            # Display top 15 features\n",
    "            print(f\"\\nüîç Top 15 Features for {best_model_name}:\")\n",
    "            print(feature_importance.head(15))\n",
    "\n",
    "            # Visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = feature_importance.head(15)\n",
    "\n",
    "            plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Top 15 Feature Importance - {best_model_name}', fontweight='bold')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Feature importance insights\n",
    "            print(f\"\\nüí° Feature Importance Insights:\")\n",
    "            print(f\"‚Ä¢ Most important feature: {feature_importance.iloc[0]['Feature']}\")\n",
    "            print(f\"‚Ä¢ Top 5 features account for {feature_importance.head(5)['Importance'].sum()/feature_importance['Importance'].sum():.1%} of total importance\")\n",
    "            print(f\"‚Ä¢ Financial features in top 10: {sum(1 for f in feature_importance.head(10)['Feature'] if 'Amt' in f or 'sum' in f or 'mean' in f)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting feature importance: {e}\")"
   ],
   "id": "ffd413b72d06cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cross-Validation Analysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "import numpy as np\n",
    "print(\"=== Cross-Validation Analysis ===\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    from sklearn.model_selection import cross_validate\n",
    "\n",
    "    # Perform cross-validation for the best model\n",
    "    best_model_name = results_df.loc[results_df['F1'].idxmax()]['Model']\n",
    "    best_pipeline = trained_models[best_model_name]\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = ['precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "    print(f\"\\nPerforming 5-fold cross-validation for {best_model_name}...\")\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    try:\n",
    "        cv_results = cross_validate(\n",
    "            best_pipeline, X_train, y_train,\n",
    "            cv=cv, scoring=scoring,\n",
    "            return_train_score=True, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Calculate statistics\n",
    "        cv_stats = {}\n",
    "        for metric in scoring:\n",
    "            test_scores = cv_results[f'test_{metric}']\n",
    "            train_scores = cv_results[f'train_{metric}']\n",
    "\n",
    "            cv_stats[metric] = {\n",
    "                'test_mean': np.mean(test_scores),\n",
    "                'test_std': np.std(test_scores),\n",
    "                'train_mean': np.mean(train_scores),\n",
    "                'train_std': np.std(train_scores),\n",
    "                'overfit': np.mean(train_scores) - np.mean(test_scores)\n",
    "            }\n",
    "\n",
    "        # Display results\n",
    "        print(f\"\\nüìä Cross-Validation Results for {best_model_name}:\")\n",
    "        print(\"=\" * 65)\n",
    "        print(f\"{'Metric':<12} {'Test Mean':<10} {'Test Std':<10} {'Train Mean':<11} {'Overfitting':<12}\")\n",
    "        print(\"-\" * 65)\n",
    "\n",
    "        for metric, stats in cv_stats.items():\n",
    "            print(\n",
    "                f\"{metric.upper():<12} {stats['test_mean']:.3f}      {stats['test_std']:.3f}      {stats['train_mean']:.3f}       {stats['overfit']:.3f}\")\n",
    "\n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for idx, metric in enumerate(scoring):\n",
    "            test_scores = cv_results[f'test_{metric}']\n",
    "            train_scores = cv_results[f'train_{metric}']\n",
    "\n",
    "            x_pos = range(1, len(test_scores) + 1)\n",
    "            axes[idx].plot(x_pos, test_scores, 'o-', label='Test', color='blue', alpha=0.7)\n",
    "            axes[idx].plot(x_pos, train_scores, 's-', label='Train', color='red', alpha=0.7)\n",
    "            axes[idx].axhline(y=np.mean(test_scores), color='blue', linestyle='--', alpha=0.5)\n",
    "            axes[idx].axhline(y=np.mean(train_scores), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "            axes[idx].set_xlabel('Fold')\n",
    "            axes[idx].set_ylabel(metric.upper())\n",
    "            axes[idx].set_title(f'{metric.upper()} Across CV Folds')\n",
    "            axes[idx].legend()\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "            axes[idx].set_xticks(x_pos)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Model stability assessment\n",
    "        f1_std = cv_stats['f1']['test_std']\n",
    "        stability_assessment = \"Stable\" if f1_std < 0.05 else \"Moderate\" if f1_std < 0.10 else \"Unstable\"\n",
    "\n",
    "        print(f\"\\nüéØ Model Stability Assessment:\")\n",
    "        print(f\"‚Ä¢ F1-Score Std Dev: {f1_std:.4f}\")\n",
    "        print(f\"‚Ä¢ Stability Rating: {stability_assessment}\")\n",
    "        print(f\"‚Ä¢ Max Overfitting: {max(cv_stats[m]['overfit'] for m in scoring):.4f} (F1)\")\n",
    "\n",
    "        if f1_std > 0.05:\n",
    "            print(\"‚ö†Ô∏è Consider additional regularization or feature selection\")\n",
    "        else:\n",
    "            print(\"‚úÖ Model shows good stability across different data splits\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cross-validation failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models available for cross-validation\")"
   ],
   "id": "6f32b4202ffa9baa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Threshold Optimization for Best Model\n",
    "if len(results_df) > 0:\n",
    "    print(\"=== Threshold Optimization ===\")\n",
    "\n",
    "    # Get best model\n",
    "    best_model_name = results_df.loc[results_df['F1'].idxmax()]['Model']\n",
    "    best_pipeline = trained_models[best_model_name]\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Test different thresholds (include 0.5 explicitly)\n",
    "    thresholds = np.unique(np.concatenate([\n",
    "        np.arange(0.1, 0.9, 0.05),\n",
    "        [0.5]  # Ensure 0.5 is included\n",
    "    ]))\n",
    "    thresholds = np.sort(thresholds)\n",
    "    threshold_results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "        precision = precision_score(y_test, y_pred_thresh)\n",
    "        recall = recall_score(y_test, y_pred_thresh)\n",
    "        f1 = f1_score(y_test, y_pred_thresh)\n",
    "\n",
    "        # Calculate business metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
    "        investigation_load = tp + fp  # Total cases to investigate\n",
    "        fraud_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        threshold_results.append({\n",
    "            'Threshold': threshold,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'Investigation_Load': investigation_load,\n",
    "            'Fraud_Detection_Rate': fraud_detection_rate,\n",
    "            'True_Positives': tp,\n",
    "            'False_Positives': fp\n",
    "        })\n",
    "\n",
    "    threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "    # Find optimal thresholds for different objectives\n",
    "    best_f1_idx = threshold_df['F1'].idxmax()\n",
    "    best_precision_idx = threshold_df['Precision'].idxmax()\n",
    "\n",
    "    # Business efficiency: maximize fraud detection while limiting investigations\n",
    "    threshold_df['Efficiency'] = threshold_df['Fraud_Detection_Rate'] / (threshold_df['Investigation_Load'] / len(y_test))\n",
    "    best_efficiency_idx = threshold_df['Efficiency'].idxmax()\n",
    "\n",
    "    print(f\"\\nüìä Threshold Optimization Results for {best_model_name}:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Objective':<20} {'Threshold':<12} {'Precision':<12} {'Recall':<10} {'F1':<10} {'Investigations':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Best F1':<20} {threshold_df.iloc[best_f1_idx]['Threshold']:<12.2f} {threshold_df.iloc[best_f1_idx]['Precision']:<12.3f} {threshold_df.iloc[best_f1_idx]['Recall']:<10.3f} {threshold_df.iloc[best_f1_idx]['F1']:<10.3f} {threshold_df.iloc[best_f1_idx]['Investigation_Load']:<15.0f}\")\n",
    "    print(f\"{'Best Precision':<20} {threshold_df.iloc[best_precision_idx]['Threshold']:<12.2f} {threshold_df.iloc[best_precision_idx]['Precision']:<12.3f} {threshold_df.iloc[best_precision_idx]['Recall']:<10.3f} {threshold_df.iloc[best_precision_idx]['F1']:<10.3f} {threshold_df.iloc[best_precision_idx]['Investigation_Load']:<15.0f}\")\n",
    "    print(f\"{'Best Efficiency':<20} {threshold_df.iloc[best_efficiency_idx]['Threshold']:<12.2f} {threshold_df.iloc[best_efficiency_idx]['Precision']:<12.3f} {threshold_df.iloc[best_efficiency_idx]['Recall']:<10.3f} {threshold_df.iloc[best_efficiency_idx]['F1']:<10.3f} {threshold_df.iloc[best_efficiency_idx]['Investigation_Load']:<15.0f}\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Metrics vs Threshold\n",
    "    axes[0].plot(threshold_df['Threshold'], threshold_df['Precision'], 'b-', label='Precision', linewidth=2)\n",
    "    axes[0].plot(threshold_df['Threshold'], threshold_df['Recall'], 'r-', label='Recall', linewidth=2)\n",
    "    axes[0].plot(threshold_df['Threshold'], threshold_df['F1'], 'g-', label='F1-Score', linewidth=2)\n",
    "\n",
    "    # Mark optimal points\n",
    "    axes[0].scatter(threshold_df.iloc[best_f1_idx]['Threshold'], threshold_df.iloc[best_f1_idx]['F1'],\n",
    "                   color='green', s=100, zorder=5, label='Optimal F1')\n",
    "\n",
    "    axes[0].set_xlabel('Threshold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_title('Performance Metrics vs Threshold', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Investigation Load vs Fraud Detection Rate\n",
    "    axes[1].scatter(threshold_df['Investigation_Load'], threshold_df['Fraud_Detection_Rate'],\n",
    "                   c=threshold_df['F1'], cmap='viridis', s=60, alpha=0.7)\n",
    "\n",
    "    # Mark efficient point\n",
    "    axes[1].scatter(threshold_df.iloc[best_efficiency_idx]['Investigation_Load'],\n",
    "                   threshold_df.iloc[best_efficiency_idx]['Fraud_Detection_Rate'],\n",
    "                   color='red', s=100, zorder=5, label='Most Efficient')\n",
    "\n",
    "    axes[1].set_xlabel('Investigation Load (Cases)')\n",
    "    axes[1].set_ylabel('Fraud Detection Rate')\n",
    "    axes[1].set_title('Investigation Efficiency Trade-off', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "    cbar.set_label('F1-Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Threshold Recommendations:\")\n",
    "    print(f\"‚Ä¢ For balanced performance: {threshold_df.iloc[best_f1_idx]['Threshold']:.2f} (F1-optimized)\")\n",
    "    print(f\"‚Ä¢ For high confidence alerts: {threshold_df.iloc[best_precision_idx]['Threshold']:.2f} (Precision-optimized)\")\n",
    "    print(f\"‚Ä¢ For resource efficiency: {threshold_df.iloc[best_efficiency_idx]['Threshold']:.2f} (Investigation-optimized)\")\n",
    "\n",
    "    # Safely check for default threshold (0.5)\n",
    "    try:\n",
    "        # Try to find exact match first\n",
    "        default_threshold_mask = (threshold_df['Threshold'] == 0.5)\n",
    "        if default_threshold_mask.any():\n",
    "            print(f\"‚Ä¢ Default threshold (0.5) F1-Score: {threshold_df[default_threshold_mask]['F1'].iloc[0]:.3f}\")\n",
    "        else:\n",
    "            # Find the closest threshold to 0.5\n",
    "            closest_idx = np.argmin(np.abs(threshold_df['Threshold'] - 0.5))\n",
    "            closest_threshold = threshold_df.iloc[closest_idx]['Threshold']\n",
    "            closest_f1 = threshold_df.iloc[closest_idx]['F1']\n",
    "            print(f\"‚Ä¢ Closest to default threshold ({closest_threshold:.2f}) F1-Score: {closest_f1:.3f}\")\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print(f\"‚Ä¢ Default threshold comparison: Unable to compute (threshold range: {threshold_df['Threshold'].min():.2f}-{threshold_df['Threshold'].max():.2f})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models available for threshold optimization\")"
   ],
   "id": "e7dfafe61410dd78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final recommendations and business impact analysis\n",
    "if len(results_df) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"HEALTHCARE FRAUD DETECTION - COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    best_model = results_df.loc[results_df['F1'].idxmax()]\n",
    "\n",
    "    print(f\"\\nüéØ RECOMMENDED MODEL: {best_model['Model']}\")\n",
    "    print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
    "    print(f\"  ‚Ä¢ F1-Score: {best_model['F1']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Precision: {best_model['Precision']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Recall: {best_model['Recall']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ PR-AUC: {best_model['PR_AUC']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ ROC-AUC: {best_model['ROC_AUC']:.4f}\")\n",
    "\n",
    "    fraud_detected = int(best_model['Recall'] * (y_test == 1).sum())\n",
    "    total_fraud = (y_test == 1).sum()\n",
    "    total_providers = len(y_test)\n",
    "\n",
    "    print(f\"\\nüìà BUSINESS IMPACT ANALYSIS:\")\n",
    "    print(f\"  ‚Ä¢ Total providers evaluated: {total_providers:,}\")\n",
    "    print(f\"  ‚Ä¢ Actual fraud cases: {total_fraud}\")\n",
    "    print(f\"  ‚Ä¢ Fraud cases detected: {fraud_detected} out of {total_fraud}\")\n",
    "    print(f\"  ‚Ä¢ Detection rate: {best_model['Recall']:.1%}\")\n",
    "    print(f\"  ‚Ä¢ Precision rate: {best_model['Precision']:.1%}\")\n",
    "    print(f\"  ‚Ä¢ False positive rate: {((1-best_model['Precision']) * fraud_detected/best_model['Precision'] if best_model['Precision'] > 0 else 0):.0f} unnecessary investigations\")\n",
    "\n",
    "\n",
    "    # Calculate potential savings\n",
    "    avg_fraud_loss = 50000  # Assume $50k average fraud loss per case\n",
    "    investigation_cost = 5000  # Assume $5k per investigation\n",
    "\n",
    "    fraud_prevented = fraud_detected * avg_fraud_loss\n",
    "    investigation_costs = (fraud_detected / best_model['Precision']) * investigation_cost if best_model['Precision'] > 0 else 0\n",
    "    net_savings = fraud_prevented - investigation_costs\n",
    "\n",
    "    print(f\"\\nüí∞ ESTIMATED FINANCIAL IMPACT (Annual):\")\n",
    "    print(f\"  ‚Ä¢ Fraud prevented: ${fraud_prevented:,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Investigation costs: ${investigation_costs:,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Net savings: ${net_savings:,.0f}\")\n",
    "    print(f\"  ‚Ä¢ ROI: {(net_savings/investigation_costs*100) if investigation_costs > 0 else 0:.1f}%\")\n",
    "\n",
    "    print(f\"\\nüèÜ CLASS IMBALANCE STRATEGY EFFECTIVENESS:\")\n",
    "    print(f\"  ‚Ä¢ Approach: Class Weighting\")\n",
    "    ratio = int((y == 0).sum()) / int((y == 1).sum())\n",
    "    print(f\"  ‚Ä¢ Original class ratio: {ratio:.1f}:1\")\n",
    "    print(f\"  ‚Ä¢ Effective handling: {'‚úÖ Yes' if best_model['F1'] > 0.7 else '‚ö†Ô∏è Moderate' if best_model['F1'] > 0.5 else '‚ùå Poor'}\")\n",
    "    print(f\"  ‚Ä¢ Maintains data integrity: ‚úÖ\")\n",
    "    print(f\"  ‚Ä¢ Computationally efficient: ‚úÖ\")\n",
    "\n",
    "    print(f\"\\nüîß PRODUCTION DEPLOYMENT RECOMMENDATIONS:\")\n",
    "    print(f\"  1. Primary Model: {best_model['Model']} with class weighting\")\n",
    "    print(f\"  2. Optimal Threshold: Use threshold optimization results\")\n",
    "    print(f\"  3. Monitoring: Track precision/recall drift monthly\")\n",
    "    print(f\"  4. Retraining: Quarterly with new fraud patterns\")\n",
    "    print(f\"  5. Interpretability: Implement feature importance tracking\")\n",
    "    print(f\"  6. Alerts: Set up performance degradation warnings\")\n",
    "    print(f\"  7. Compliance: Maintain audit trail for regulatory review\")\n",
    "\n",
    "    print(f\"\\nüìã MODEL VALIDATION CHECKLIST:\")\n",
    "    validation_checks = {\n",
    "        \"Cross-validation performed\": \"‚úÖ\",\n",
    "        \"Class imbalance addressed\": \"‚úÖ\",\n",
    "        \"Multiple algorithms compared\": \"‚úÖ\",\n",
    "        \"Feature importance analyzed\": \"‚úÖ\",\n",
    "        \"Threshold optimization done\": \"‚úÖ\",\n",
    "        \"Business metrics calculated\": \"‚úÖ\",\n",
    "        \"Overfitting checked\": \"‚úÖ\" if 'cv_stats' in locals() else \"‚ö†Ô∏è\"\n",
    "    }\n",
    "\n",
    "    for check, status in validation_checks.items():\n",
    "        print(f\"  {status} {check}\")\n",
    "\n",
    "    # Save comprehensive results\n",
    "    results_df.to_csv('../data/model_results.csv', index=False)\n",
    "\n",
    "    # Create deployment summary\n",
    "    deployment_summary = {\n",
    "        'best_model': best_model['Model'],\n",
    "        'f1_score': best_model['F1'],\n",
    "        'precision': best_model['Precision'],\n",
    "        'recall': best_model['Recall'],\n",
    "        'recommended_threshold': 0.5,  # Update with optimal threshold if calculated\n",
    "        'deployment_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "        'total_features': X.shape[1],\n",
    "        'training_size': len(y_train),\n",
    "        'test_size': len(y_test)\n",
    "    }\n",
    "\n",
    "    # Save deployment info\n",
    "    import json\n",
    "    with open('../data/deployment_summary.json', 'w') as f:\n",
    "        json.dump(deployment_summary, f, indent=2)\n",
    "\n",
    "    print(f\"\\nüíæ ARTIFACTS SAVED:\")\n",
    "    print(f\"  ‚Ä¢ Model comparison: ../data/model_results.csv\")\n",
    "    print(f\"  ‚Ä¢ Deployment summary: ../data/deployment_summary.json\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ ANALYSIS COMPLETE - MODEL READY FOR DEPLOYMENT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models were successfully trained. Please check your data and try again.\")"
   ],
   "id": "6fa559758e40df54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
