{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9dd37e",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d407e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared dataset from feature engineering notebook\n",
    "try:\n",
    "    # Load the final dataset created in the previous notebook\n",
    "    final_dataset = pd.read_csv('../data/provider_level.csv', index_col='Provider')\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {final_dataset.shape}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = final_dataset.drop(['PotentialFraud', 'PotentialFraud_numeric'], axis=1)\n",
    "    y = final_dataset['PotentialFraud_numeric']\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target distribution:\")\n",
    "    print(f\"  Non-fraud: {(y == 0).sum()} ({(y == 0).mean():.1%})\")\n",
    "    print(f\"  Fraud: {(y == 1).sum()} ({(y == 1).mean():.1%})\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Please run the data exploration and feature engineering notebook first.\")\n",
    "    print(\"Expected file: ../data/provider_level.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7085f4e",
   "metadata": {},
   "source": [
    "## 2. Machine Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, precision_recall_curve,\n",
    "    roc_curve, auc, precision_score, recall_score, f1_score, \n",
    "    accuracy_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available, skipping XGBoost models\")\n",
    "    xgb = None\n",
    "\n",
    "print(\"ML libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531311c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for modeling\n",
    "print(\"=== Data Preparation for Modeling ===\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Calculate class weights and scale_pos_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26301b85",
   "metadata": {},
   "source": [
    "## 3. Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, average='binary'),\n",
    "        'Recall': recall_score(y_true, y_pred, average='binary'),\n",
    "        'F1': f1_score(y_true, y_pred, average='binary'),\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_pred_proba),\n",
    "        'PR_AUC': average_precision_score(y_true, y_pred_proba)\n",
    "    }\n",
    "\n",
    "# Define models with class weighting\n",
    "models = {\n",
    "    'Logistic_Regression': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000))\n",
    "    ]),\n",
    "    'Random_Forest': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100))\n",
    "    ]),\n",
    "    'Decision_Tree': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=10))\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(class_weight='balanced', random_state=42, probability=True))\n",
    "    ])\n",
    "}\n",
    "\n",
    "if xgb is not None:\n",
    "    models['XGBoost'] = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss'))\n",
    "    ])\n",
    "\n",
    "print(f\"‚úÖ {len(models)} models defined: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "print(\"=== Model Training and Evaluation ===\")\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_test = pipeline.predict(X_test)\n",
    "        y_pred_proba_test = pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(y_test, y_pred_test, y_pred_proba_test)\n",
    "        metrics['Model'] = name\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Store trained model\n",
    "        trained_models[name] = pipeline\n",
    "        \n",
    "        print(f\"  F1: {metrics['F1']:.4f}, PR-AUC: {metrics['PR_AUC']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error training {name}: {e}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Comparison Results ===\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fcbe4",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6648713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "if len(results_df) > 0:\n",
    "    # Performance metrics heatmap\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Performance heatmap\n",
    "    metrics_cols = ['Precision', 'Recall', 'F1', 'ROC_AUC', 'PR_AUC']\n",
    "    heatmap_data = results_df.set_index('Model')[metrics_cols]\n",
    "    \n",
    "    sns.heatmap(heatmap_data.T, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                ax=axes[0,0], cbar_kws={'label': 'Score'})\n",
    "    axes[0,0].set_title('Model Performance Heatmap', fontweight='bold')\n",
    "    \n",
    "    # 2. F1 Score comparison\n",
    "    f1_scores = results_df.sort_values('F1', ascending=True)\n",
    "    axes[0,1].barh(f1_scores['Model'], f1_scores['F1'], color='skyblue', alpha=0.7)\n",
    "    axes[0,1].set_xlabel('F1 Score')\n",
    "    axes[0,1].set_title('F1 Score Comparison', fontweight='bold')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Precision vs Recall\n",
    "    axes[1,0].scatter(results_df['Precision'], results_df['Recall'], \n",
    "                     c=results_df['F1'], cmap='viridis', s=100, alpha=0.7)\n",
    "    for i, model in enumerate(results_df['Model']):\n",
    "        axes[1,0].annotate(model, \n",
    "                          (results_df.iloc[i]['Precision'], results_df.iloc[i]['Recall']),\n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    axes[1,0].set_xlabel('Precision')\n",
    "    axes[1,0].set_ylabel('Recall')\n",
    "    axes[1,0].set_title('Precision vs Recall Trade-off', fontweight='bold')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ROC vs PR AUC\n",
    "    axes[1,1].scatter(results_df['ROC_AUC'], results_df['PR_AUC'], \n",
    "                     c=results_df['F1'], cmap='plasma', s=100, alpha=0.7)\n",
    "    for i, model in enumerate(results_df['Model']):\n",
    "        axes[1,1].annotate(model, \n",
    "                          (results_df.iloc[i]['ROC_AUC'], results_df.iloc[i]['PR_AUC']),\n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    axes[1,1].set_xlabel('ROC AUC')\n",
    "    axes[1,1].set_ylabel('PR AUC')\n",
    "    axes[1,1].set_title('ROC AUC vs PR AUC', fontweight='bold')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best model summary\n",
    "    best_f1_model = results_df.loc[results_df['F1'].idxmax()]\n",
    "    print(\"\\n=== BEST MODEL BY F1-SCORE ===\")\n",
    "    print(f\"Model: {best_f1_model['Model']}\")\n",
    "    print(f\"F1-Score: {best_f1_model['F1']:.4f}\")\n",
    "    print(f\"Precision: {best_f1_model['Precision']:.4f}\")\n",
    "    print(f\"Recall: {best_f1_model['Recall']:.4f}\")\n",
    "    print(f\"PR-AUC: {best_f1_model['PR_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310f62e",
   "metadata": {},
   "source": [
    "## 5. Results and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4db95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations\n",
    "if len(results_df) > 0:\n",
    "    print(\"=\"*60)\n",
    "    print(\"HEALTHCARE FRAUD DETECTION - FINAL RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_model = results_df.loc[results_df['F1'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nüéØ RECOMMENDED MODEL: {best_model['Model']}\")\n",
    "    print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
    "    print(f\"  ‚Ä¢ F1-Score: {best_model['F1']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Precision: {best_model['Precision']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Recall: {best_model['Recall']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ PR-AUC: {best_model['PR_AUC']:.4f}\")\n",
    "    \n",
    "    fraud_detected = int(best_model['Recall'] * (y_test == 1).sum())\n",
    "    total_fraud = (y_test == 1).sum()\n",
    "    \n",
    "    print(f\"\\nüìà BUSINESS IMPACT:\")\n",
    "    print(f\"  ‚Ä¢ Fraud cases detected: {fraud_detected} out of {total_fraud}\")\n",
    "    print(f\"  ‚Ä¢ Detection rate: {best_model['Recall']:.1%}\")\n",
    "    print(f\"  ‚Ä¢ Precision rate: {best_model['Precision']:.1%}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ CLASS IMBALANCE STRATEGY: Class Weighting\")\n",
    "    print(f\"  ‚Ä¢ Maintains original data distribution\")\n",
    "    print(f\"  ‚Ä¢ Avoids synthetic data problems\")\n",
    "    print(f\"  ‚Ä¢ Computationally efficient\")\n",
    "    \n",
    "    print(f\"\\nüîß DEPLOYMENT RECOMMENDATIONS:\")\n",
    "    print(f\"  1. Implement {best_model['Model']} as primary detection system\")\n",
    "    print(f\"  2. Use class weighting for imbalance handling\")\n",
    "    print(f\"  3. Set threshold based on business cost considerations\")\n",
    "    print(f\"  4. Regular model retraining (quarterly recommended)\")\n",
    "    print(f\"  5. Monitor feature importance for model transparency\")\n",
    "    print(f\"  6. Implement alerts for significant performance drift\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('../data/model_results.csv', index=False)\n",
    "    print(f\"\\nüíæ Results saved to: ../data/model_results.csv\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models were successfully trained. Please check your data and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e3e5f",
   "metadata": {},
   "source": [
    "## 6. Model Persistence (Optional)\n",
    "\n",
    "Save the best performing model for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "if len(results_df) > 0:\n",
    "    import joblib\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model_name = results_df.loc[results_df['F1'].idxmax()]['Model']\n",
    "    best_pipeline = trained_models[best_model_name]\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f'../data/best_fraud_detection_model_{best_model_name.lower()}.pkl'\n",
    "    joblib.dump(best_pipeline, model_path)\n",
    "    \n",
    "    print(f\"‚úÖ Best model ({best_model_name}) saved to: {model_path}\")\n",
    "    \n",
    "    # Save feature names for future prediction\n",
    "    feature_names = X.columns.tolist()\n",
    "    joblib.dump(feature_names, '../data/feature_names.pkl')\n",
    "    \n",
    "    print(f\"‚úÖ Feature names saved to: ../data/feature_names.pkl\")\n",
    "    print(f\"\\nüìã To use this model for prediction:\")\n",
    "    print(f\"   model = joblib.load('{model_path}')\")\n",
    "    print(f\"   features = joblib.load('../data/feature_names.pkl')\")\n",
    "    print(f\"   predictions = model.predict(new_data)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No trained models available for saving.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
