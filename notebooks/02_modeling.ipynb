{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dfc3fa",
   "metadata": {},
   "source": [
    "# Healthcare Provider Fraud Detection: Machine Learning Modeling\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### 1. Fraud Detection as a Classification Problem\n",
    "\n",
    "Healthcare fraud detection is fundamentally a **binary classification problem** where we predict whether a provider is fraudulent (`1`) or legitimate (`0`). This presents unique challenges that distinguish it from standard classification tasks:\n",
    "\n",
    "#### 1.1 Problem Characteristics\n",
    "- **Severe Class Imbalance**: ~10% fraud rate creates significant bias toward majority class\n",
    "- **High-Stakes Decisions**: False positives damage provider reputation; false negatives allow fraud to continue\n",
    "- **Interpretability Requirements**: Regulators need explainable predictions for investigations\n",
    "- **Evolving Fraud Patterns**: Fraudsters adapt, requiring robust and generalizable models\n",
    "\n",
    "#### 1.2 Business Context\n",
    "- **Cost of Investigation**: Limited resources require prioritizing high-confidence cases\n",
    "- **Regulatory Compliance**: Models must align with healthcare regulations and audit requirements\n",
    "- **Temporal Dynamics**: Fraud patterns evolve, necessitating regular model updates\n",
    "\n",
    "### 2. Class Imbalance Handling Strategies\n",
    "\n",
    "#### 2.1 Class Weighting (Chosen Approach)\n",
    "**Theory**: Assigns higher penalties to misclassifying minority class samples during training.\n",
    "\n",
    "**Mathematical Foundation**:\n",
    "```\n",
    "Class Weight = n_samples / (n_classes × n_samples_class)\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Preserves original data distribution\n",
    "- Computationally efficient\n",
    "- No synthetic data generation risks\n",
    "- Works well with ensemble methods\n",
    "\n",
    "**Limitations**:\n",
    "- May increase false positive rate\n",
    "- Requires careful threshold tuning\n",
    "\n",
    "#### 2.2 Alternative Approaches (Considered but not implemented)\n",
    "- **SMOTE**: Risk of generating unrealistic synthetic fraud patterns\n",
    "- **Undersampling**: Loss of valuable information from majority class\n",
    "- **Cost-Sensitive Learning**: Requires domain-specific cost matrices\n",
    "\n",
    "### 3. Algorithm Selection Framework\n",
    "\n",
    "#### 3.1 Evaluation Criteria\n",
    "1. **Performance on Imbalanced Data**: Emphasis on F1-score and PR-AUC\n",
    "2. **Interpretability**: Essential for regulatory compliance\n",
    "3. **Robustness**: Stability across different data subsets\n",
    "4. **Computational Efficiency**: Scalability to large datasets\n",
    "\n",
    "#### 3.2 Selected Algorithms\n",
    "\n",
    "**Logistic Regression**\n",
    "- **Theory**: Linear decision boundary with probabilistic output\n",
    "- **Advantages**: Highly interpretable, fast training, good baseline\n",
    "- **Use Case**: Benchmark model and regulatory explanations\n",
    "\n",
    "**Random Forest**\n",
    "- **Theory**: Ensemble of decision trees with voting mechanism\n",
    "- **Advantages**: Handles mixed data types, built-in feature importance, robust to outliers\n",
    "- **Use Case**: Primary candidate for production deployment\n",
    "\n",
    "**Decision Tree**\n",
    "- **Theory**: Hierarchical splitting rules for classification\n",
    "- **Advantages**: Maximum interpretability, handles non-linear patterns\n",
    "- **Use Case**: Explanation model for specific cases\n",
    "\n",
    "**Support Vector Machine (SVM)**\n",
    "- **Theory**: Maximum margin classification with kernel trick\n",
    "- **Advantages**: Effective in high-dimensional spaces, memory efficient\n",
    "- **Use Case**: Complex pattern detection with RBF kernel\n",
    "\n",
    "**XGBoost (Optional)**\n",
    "- **Theory**: Gradient boosting with advanced regularization\n",
    "- **Advantages**: State-of-the-art performance, built-in class weighting\n",
    "- **Use Case**: Performance benchmark if computational resources allow\n",
    "\n",
    "### 4. Evaluation Methodology\n",
    "\n",
    "#### 4.1 Metrics Hierarchy\n",
    "1. **Primary**: F1-Score (harmonic mean of precision/recall)\n",
    "2. **Secondary**: PR-AUC (area under precision-recall curve)\n",
    "3. **Supplementary**: ROC-AUC, Precision, Recall\n",
    "\n",
    "#### 4.2 Why F1-Score Priority?\n",
    "- Balances precision (false positive control) and recall (fraud detection)\n",
    "- More meaningful than accuracy for imbalanced datasets\n",
    "- Aligns with business objectives of effective fraud detection\n",
    "\n",
    "#### 4.3 Cross-Validation Strategy\n",
    "- **Stratified K-Fold**: Preserves class distribution across folds\n",
    "- **K=5**: Balance between bias-variance trade-off and computational cost\n",
    "\n",
    "### 5. Model Interpretability Requirements\n",
    "\n",
    "#### 5.1 Regulatory Compliance\n",
    "- Feature importance rankings\n",
    "- Decision path explanations\n",
    "- Confidence score interpretation\n",
    "\n",
    "#### 5.2 Business Stakeholder Needs\n",
    "- Clear feature contribution analysis\n",
    "- Fraud pattern identification\n",
    "- Risk scoring methodology\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9dd37e",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "0aac3c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:12:01.640549Z",
     "start_time": "2025-12-03T12:12:01.634829Z"
    }
   },
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "0d407e7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:15:53.774122Z",
     "start_time": "2025-12-03T12:15:53.686289Z"
    }
   },
   "source": [
    "# Load the prepared dataset from feature engineering notebook\n",
    "try:\n",
    "    # Load the final dataset created in the previous notebook\n",
    "    final_dataset = pd.read_csv('../data/provider_level.csv', index_col='Provider')\n",
    "    \n",
    "    print(f\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {final_dataset.shape}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = final_dataset.drop(['PotentialFraud', 'PotentialFraud_numeric'], axis=1)\n",
    "    y = final_dataset['PotentialFraud_numeric']\n",
    "\n",
    "    X.to_csv('../data/X.csv', index=False)\n",
    "    y.to_csv('../data/y.csv', index=False)\n",
    "\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target distribution:\")\n",
    "    print(f\"  Non-fraud: {(y == 0).sum()} ({(y == 0).mean():.1%})\")\n",
    "    print(f\"  Fraud: {(y == 1).sum()} ({(y == 1).mean():.1%})\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Dataset not found. Please run the data exploration and feature engineering notebook first.\")\n",
    "    print(\"Expected file: ../data/provider_level.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully!\n",
      "Dataset shape: (5410, 35)\n",
      "Features shape: (5410, 33)\n",
      "Target distribution:\n",
      "  Non-fraud: 4904 (90.6%)\n",
      "  Fraud: 506 (9.4%)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b7085f4e",
   "metadata": {},
   "source": [
    "## 2. Machine Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "6d61961d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:15:35.368487Z",
     "start_time": "2025-12-03T12:15:35.094394Z"
    }
   },
   "source": [
    "# Import ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, precision_recall_curve,\n",
    "    roc_curve, auc, precision_score, recall_score, f1_score, \n",
    "    accuracy_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import xgboost as xgb\n",
    "print(\"✅ XGBoost available\")\n",
    "\n",
    "print(\"ML libraries loaded successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost available\n",
      "ML libraries loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "531311c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:15:56.243390Z",
     "start_time": "2025-12-03T12:15:56.163710Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data preparation for modeling\n",
    "print(\"=== Data Preparation for Modeling ===\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.to_csv('../data/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/y_test.csv', index=False)\n",
    "\n",
    "# Calculate class weights and scale_pos_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"✅ Data preparation complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Preparation for Modeling ===\n",
      "Training set: (4328, 33), Test set: (1082, 33)\n",
      "Class weights: {np.int64(0): np.float64(0.5516186591893959), np.int64(1): np.float64(5.3432098765432094)}\n",
      "Scale pos weight: 9.69\n",
      "✅ Data preparation complete!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "26301b85",
   "metadata": {},
   "source": [
    "## 3. Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "294f2d26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:16:14.396203Z",
     "start_time": "2025-12-03T12:16:14.387884Z"
    }
   },
   "source": [
    "# Model evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, average='binary'),\n",
    "        'Recall': recall_score(y_true, y_pred, average='binary'),\n",
    "        'F1': f1_score(y_true, y_pred, average='binary'),\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_pred_proba),\n",
    "        'PR_AUC': average_precision_score(y_true, y_pred_proba)\n",
    "    }\n",
    "\n",
    "# Define models with class weighting\n",
    "models = {\n",
    "    'Logistic_Regression': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000))\n",
    "    ]),\n",
    "    'Random_Forest': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100))\n",
    "    ]),\n",
    "    'Decision_Tree': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=10))\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(class_weight='balanced', random_state=42, probability=True))\n",
    "    ])\n",
    "}\n",
    "\n",
    "if xgb is not None:\n",
    "    models['XGBoost'] = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss'))\n",
    "    ])\n",
    "\n",
    "print(f\"✅ {len(models)} models defined: {list(models.keys())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 5 models defined: ['Logistic_Regression', 'Random_Forest', 'Decision_Tree', 'SVM', 'XGBoost']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "d452a9b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T12:25:52.807591Z",
     "start_time": "2025-12-03T12:25:49.666433Z"
    }
   },
   "source": [
    "# Train and evaluate all models\n",
    "import joblib\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "results = []\n",
    "trained_models = {}\n",
    "print(\"=== Model Training and Evaluation ===\")\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        #save traained models\n",
    "        joblib.dump(pipeline, f'../models/{name}.pkl')\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_test = pipeline.predict(X_test)\n",
    "        y_pred_proba_test = pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(y_test, y_pred_test, y_pred_proba_test)\n",
    "        metrics['Model'] = name\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Store trained model\n",
    "        trained_models[name] = pipeline\n",
    "        \n",
    "        print(f\"  F1: {metrics['F1']:.4f}, PR-AUC: {metrics['PR_AUC']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error training {name}: {e}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "results_df.to_csv('../data/model_results.csv', index=False)\n",
    "print(\"\\n=== Model Comparison Results ===\")\n",
    "print(results_df.round(4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Training and Evaluation ===\n",
      "\n",
      "Training Logistic_Regression...\n",
      "  F1: 0.6298, PR-AUC: 0.7872\n",
      "\n",
      "Training Random_Forest...\n",
      "  F1: 0.7011, PR-AUC: 0.7847\n",
      "\n",
      "Training Decision_Tree...\n",
      "  F1: 0.6154, PR-AUC: 0.5187\n",
      "\n",
      "Training SVM...\n",
      "  F1: 0.6186, PR-AUC: 0.5786\n",
      "\n",
      "Training XGBoost...\n",
      "  F1: 0.7129, PR-AUC: 0.7841\n",
      "   Accuracy  Precision    Recall        F1   ROC_AUC    PR_AUC  \\\n",
      "0  0.901109   0.484043  0.900990  0.629758  0.969722  0.787230   \n",
      "1  0.951941   0.835616  0.603960  0.701149  0.969015  0.784721   \n",
      "2  0.907579   0.503145  0.792079  0.615385  0.866004  0.518690   \n",
      "3  0.897412   0.473684  0.891089  0.618557  0.952408  0.578576   \n",
      "4  0.946396   0.712871  0.712871  0.712871  0.962808  0.784057   \n",
      "\n",
      "                 Model  \n",
      "0  Logistic_Regression  \n",
      "1        Random_Forest  \n",
      "2        Decision_Tree  \n",
      "3                  SVM  \n",
      "4              XGBoost  \n",
      "\n",
      "=== Model Comparison Results ===\n",
      "   Accuracy  Precision  Recall      F1  ROC_AUC  PR_AUC                Model\n",
      "0    0.9011     0.4840  0.9010  0.6298   0.9697  0.7872  Logistic_Regression\n",
      "1    0.9519     0.8356  0.6040  0.7011   0.9690  0.7847        Random_Forest\n",
      "2    0.9076     0.5031  0.7921  0.6154   0.8660  0.5187        Decision_Tree\n",
      "3    0.8974     0.4737  0.8911  0.6186   0.9524  0.5786                  SVM\n",
      "4    0.9464     0.7129  0.7129  0.7129   0.9628  0.7841              XGBoost\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
